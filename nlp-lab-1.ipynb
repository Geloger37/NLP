{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from sklearnex import patch_sklearn\n# patch_sklearn()\n!pip install wget\n\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport sklearn.model_selection\nimport wget\nimport gensim\nfrom zipfile import ZipFile\nfrom gensim.models import Word2Vec\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:39:54.985000Z","iopub.execute_input":"2023-09-30T13:39:54.985364Z","iopub.status.idle":"2023-09-30T13:40:04.778347Z","shell.execute_reply.started":"2023-09-30T13:39:54.985332Z","shell.execute_reply":"2023-09-30T13:40:04.776910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier:\n    def __init__(self, extractor: str = 'self-trained w2v') -> None:\n        self.svm_cls = SVC()\n        self.extractor = extractor\n        \n        self.labels_to_id = {\n            'Экономика': 0, \n            'Спорт': 1, \n            'Культура': 2, \n            'Наука и техника': 3\n        }\n        self.id_to_labels = {\n            0: 'Экономика', \n            1: 'Спорт', \n            2: 'Культура', \n            3: 'Наука и техника'\n        }\n        self.random_state = 42\n        \n        if self.extractor == 'pretrained ft' and not os.path.exists('FastText'):\n            wget.download('http://vectors.nlpl.eu/repository/20/214.zip')\n            with ZipFile('214.zip') as zpfile:\n                zpfile.extractall('FastText')\n            os.remove('214.zip')\n        \n        \n    def read_file(self, filename: str = None) -> tuple[pd.DataFrame, list[str], list[int]|None]:\n        extension = os.path.splitext(filename)[1]\n        df, texts, labels = None, None, None\n        if extension == '.json':\n            df = pd.read_json(filename)\n            texts = []\n            labels = []\n            for k, v in self.labels_to_id.items():\n                for text in df[k]['texts']:\n                    texts.append( self.preprocessing(text) )\n                    labels.append(v)\n            \n        elif extension == '.csv':\n            df = pd.read_csv(filename)\n            texts = [self.preprocessing(text) for text in df['text'] ]\n        else:\n            raise Exception('Unsupported extension')\n        return (df, texts, labels)\n        \n    def preprocessing(self, text: str = None) -> str:\n        s = text.lower()\n        s = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", s)\n        s = re.sub(\"\\s+\", \" \", s)\n        s = s.strip()\n        return s\n    \n    def get_features(self, texts: list[str], stage: str = 'train') -> np.array:\n        features = []\n        if self.extractor == 'self-trained w2v':\n            if stage == 'train':\n                tokens = [text.split() for text in texts]\n                self.w2v_model = Word2Vec(sentences=tokens,\n                                          vector_size=300,\n                                          window=5,\n                                          min_count=1,\n                                          workers=os.cpu_count(),\n                                          seed=self.random_state\n                                         )\n                self.w2v_model.build_vocab(tokens)\n                self.w2v_model.train(tokens,\n                                     total_examples=self.w2v_model.corpus_count, \n                                     epochs=100, \n                                     report_delay=1\n                                    )\n\n            for text in texts:\n                vectors = []\n\n                for word in text.split():\n                    if word in self.w2v_model.wv:\n                        vector = self.w2v_model.wv[word]\n                        vectors.append(vector)\n\n                vectors = np.array(vectors)\n                feature = np.average(vectors, axis=0)\n                features.append(feature)\n\n            features = np.array(features)\n        elif self.extractor == 'pretrained ft':\n            self.ft_model = gensim.models.KeyedVectors.load('FastText/model.model')\n            self.ft_model.fill_norms(force=True)\n            \n            for text in texts:\n                vectors = []\n                \n                vectors.append(np.zeros(self.ft_model.vector_size))\n\n                for word in text.split():\n                    if word in self.ft_model.key_to_index:\n                        vector = self.ft_model[self.ft_model.key_to_index[word]]\n                        vectors.append(vector)\n\n                vectors = np.array(vectors)\n                \n                feature = np.average(vectors, axis=0)\n                features.append(feature)\n\n            features = np.array(features)\n        else:\n            raise Exception(f'Unsupported {self.extractor} extractor')\n        return features\n    \n    def fit(self, filename: str = None) -> float:\n        df, texts, labels = self.read_file(filename=filename)\n        \n        features = self.get_features(texts, stage='train')\n        \n        train_features, val_features, train_labels, val_labels = sklearn.model_selection.train_test_split(features, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=self.random_state)\n        \n        self.svm_cls.fit(train_features, train_labels)\n        \n        preds = self.svm_cls.predict(val_features)\n        return accuracy_score(val_labels, preds)\n        \n    \n    def predict(self, filename: str = None, submit: bool = False) -> np.array:\n        df, texts, _ = self.read_file(filename=filename)\n        \n        features = self.get_features(texts, stage = 'test')\n        \n        predictions = self.svm_cls.predict(features)\n        \n        if submit:\n            submission = pd.DataFrame()\n            \n            ids = []\n            labels = []\n            for i, v in enumerate(predictions):\n                ids.append(i)\n                labels.append(''.join(self.id_to_labels[v].strip().split()))\n                \n            submission['Id'] = ids\n            submission['Category'] = labels\n\n            submission.to_csv('submission.csv', index=False)\n            \n        return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:51:25.794799Z","iopub.execute_input":"2023-09-30T13:51:25.795211Z","iopub.status.idle":"2023-09-30T13:51:25.818648Z","shell.execute_reply.started":"2023-09-30T13:51:25.795181Z","shell.execute_reply":"2023-09-30T13:51:25.817380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls = Classifier(extractor='pretrained ft')\nval_accuracy = cls.fit(filename='/kaggle/input/nlp-itmo-exercise-1/archive/train_10000.json', )\nprint(val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:51:30.422428Z","iopub.execute_input":"2023-09-30T13:51:30.422848Z","iopub.status.idle":"2023-09-30T13:54:23.691048Z","shell.execute_reply.started":"2023-09-30T13:51:30.422814Z","shell.execute_reply":"2023-09-30T13:54:23.689749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls.predict(filename='/kaggle/input/nlp-itmo-exercise-1/archive/test.csv', submit=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T13:54:53.769977Z","iopub.execute_input":"2023-09-30T13:54:53.770427Z","iopub.status.idle":"2023-09-30T13:55:06.785505Z","shell.execute_reply.started":"2023-09-30T13:54:53.770390Z","shell.execute_reply":"2023-09-30T13:55:06.784353Z"},"trusted":true},"execution_count":null,"outputs":[]}]}