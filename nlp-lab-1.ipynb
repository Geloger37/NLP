{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from sklearnex import patch_sklearn\n# patch_sklearn()\n!pip install wget\n\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport sklearn.model_selection\nimport wget\nimport gensim\nimport torch\nfrom transformers import XLMRobertaTokenizer, XLMRobertaModel\nfrom zipfile import ZipFile\nfrom gensim.models import Word2Vec\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:14:08.216674Z","iopub.execute_input":"2023-09-30T19:14:08.217197Z","iopub.status.idle":"2023-09-30T19:14:37.816938Z","shell.execute_reply.started":"2023-09-30T19:14:08.217160Z","shell.execute_reply":"2023-09-30T19:14:37.815749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier:\n    def __init__(self, extractor: str = 'self-trained w2v') -> None:\n        self.svm_cls = LogisticRegression()\n        self.extractor = extractor\n        \n        self.labels_to_id = {\n            'Экономика': 0, \n            'Спорт': 1, \n            'Культура': 2, \n            'Наука и техника': 3\n        }\n        self.id_to_labels = {\n            0: 'Экономика', \n            1: 'Спорт', \n            2: 'Культура', \n            3: 'Наука и техника'\n        }\n        self.random_state = 42\n        \n        if self.extractor == 'pretrained ft' and not os.path.exists('FastText'):\n            wget.download('http://vectors.nlpl.eu/repository/20/214.zip')\n            with ZipFile('214.zip') as zpfile:\n                zpfile.extractall('FastText')\n            os.remove('214.zip')\n        if self.extractor == 'bert':\n            self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n            self.bert = XLMRobertaModel.from_pretrained('xlm-roberta-large').to(self.device)\n            self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n            \n        \n    def read_file(self, filename: str = None) -> tuple[pd.DataFrame, list[str], list[int]|None]:\n        extension = os.path.splitext(filename)[1]\n        df, texts, labels = None, None, None\n        if extension == '.json':\n            df = pd.read_json(filename)\n            texts = []\n            labels = []\n            for k, v in self.labels_to_id.items():\n                for text in df[k]['texts']:\n                    texts.append( self.preprocessing(text) )\n                    labels.append(v)\n            \n        elif extension == '.csv':\n            df = pd.read_csv(filename)\n            texts = [self.preprocessing(text) for text in df['text'] ]\n        else:\n            raise Exception('Unsupported extension')\n        return (df, texts, labels)\n        \n    def preprocessing(self, text: str = None) -> str:\n        s = text.lower()\n        s = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", s)\n        s = re.sub(\"\\s+\", \" \", s)\n        s = s.strip()\n        return s\n    \n    def get_features(self, texts: list[str], stage: str = 'train') -> np.array:\n        features = []\n        if self.extractor == 'self-trained w2v':\n            if stage == 'train':\n                tokens = [text.split() for text in texts]\n                self.w2v_model = Word2Vec(sentences=tokens,\n                                          vector_size=300,\n                                          window=5,\n                                          min_count=1,\n                                          workers=os.cpu_count(),\n                                          seed=self.random_state\n                                         )\n                self.w2v_model.build_vocab(tokens)\n                self.w2v_model.train(tokens,\n                                     total_examples=self.w2v_model.corpus_count, \n                                     epochs=100, \n                                     report_delay=1\n                                    )\n\n            for text in texts:\n                vectors = []\n\n                for word in text.split():\n                    if word in self.w2v_model.wv:\n                        vector = self.w2v_model.wv[word]\n                        vectors.append(vector)\n\n                vectors = np.array(vectors)\n                feature = np.average(vectors, axis=0)\n                features.append(feature)\n\n            features = np.array(features)\n        elif self.extractor == 'pretrained ft':\n            self.ft_model = gensim.models.KeyedVectors.load('FastText/model.model')\n            self.ft_model.fill_norms(force=True)\n            \n            for text in texts:\n                vectors = []\n                \n                vectors.append(np.zeros(self.ft_model.vector_size))\n\n                for word in text.split():\n                    if word in self.ft_model.key_to_index:\n                        vector = self.ft_model[self.ft_model.key_to_index[word]]\n                        vectors.append(vector)\n\n                vectors = np.array(vectors)\n                \n                feature = np.average(vectors, axis=0)\n                features.append(feature)\n\n            features = np.array(features)\n        elif self.extractor == 'bert':\n            for text in texts:\n                decoded = self.tokenizer.encode_plus(text,\n                                                       return_tensors='pt',\n                                                       max_length = 512,\n                                                       padding='max_length',\n                                                       add_special_tokens=True,\n                                                       truncation=True, \n                                                       return_attention_mask = True).to(self.device)\n                \n                with torch.no_grad():\n                    feature = self.bert(input_ids=decoded.input_ids, attention_mask=decoded.attention_mask).last_hidden_state.squeeze()[0] # get CLS feature token\n                    feature = feature.detach().cpu().numpy()\n                    features.append(feature)\n\n            features = np.array(features)\n        else:\n            raise Exception(f'Unsupported {self.extractor} extractor')\n        return features\n    \n    def fit(self, filename: str = None) -> float:\n        df, texts, labels = self.read_file(filename=filename)\n        \n        features = self.get_features(texts, stage='train')\n        \n        train_features, val_features, train_labels, val_labels = sklearn.model_selection.train_test_split(features, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=self.random_state)\n        \n        self.svm_cls.fit(train_features, train_labels)\n        \n        preds = self.svm_cls.predict(val_features)\n        return accuracy_score(val_labels, preds)\n        \n    \n    def predict(self, filename: str = None, submit: bool = False) -> np.array:\n        df, texts, _ = self.read_file(filename=filename)\n        \n        features = self.get_features(texts, stage = 'test')\n        \n        predictions = self.svm_cls.predict(features)\n        \n        if submit:\n            submission = pd.DataFrame()\n            \n            ids = []\n            labels = []\n            for i, v in enumerate(predictions):\n                ids.append(i)\n                labels.append(''.join(self.id_to_labels[v].strip().split()))\n                \n            submission['Id'] = ids\n            submission['Category'] = labels\n\n            submission.to_csv('submission.csv', index=False)\n            \n        return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:14:37.819067Z","iopub.execute_input":"2023-09-30T19:14:37.819457Z","iopub.status.idle":"2023-09-30T19:14:37.839729Z","shell.execute_reply.started":"2023-09-30T19:14:37.819430Z","shell.execute_reply":"2023-09-30T19:14:37.838661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls = Classifier(extractor='bert')\nval_accuracy = cls.fit(filename='/kaggle/input/nlp-itmo-exercise-1/archive/train_10000.json')\nprint(val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:14:37.840944Z","iopub.execute_input":"2023-09-30T19:14:37.841195Z","iopub.status.idle":"2023-09-30T19:15:55.635054Z","shell.execute_reply.started":"2023-09-30T19:14:37.841166Z","shell.execute_reply":"2023-09-30T19:15:55.633965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls.predict(filename='/kaggle/input/nlp-itmo-exercise-1/archive/test.csv', submit=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T16:03:49.239633Z","iopub.execute_input":"2023-09-30T16:03:49.240003Z","iopub.status.idle":"2023-09-30T16:03:59.501759Z","shell.execute_reply.started":"2023-09-30T16:03:49.239965Z","shell.execute_reply":"2023-09-30T16:03:59.500829Z"},"trusted":true},"execution_count":null,"outputs":[]}]}